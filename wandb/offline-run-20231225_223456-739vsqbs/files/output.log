{'loss': 5.0904, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.22}
{'loss': 4.6627, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.44}
{'loss': 4.4061, 'learning_rate': 1.5e-06, 'epoch': 0.67}
{'loss': 4.2775, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.89}
{'eval_loss': 3.9452619552612305, 'eval_runtime': 28.0773, 'eval_samples_per_second': 35.616, 'eval_steps_per_second': 8.904, 'epoch': 1.0}
{'loss': 4.1302, 'learning_rate': 2.5e-06, 'epoch': 1.11}
{'loss': 4.0401, 'learning_rate': 3e-06, 'epoch': 1.33}
{'loss': 3.9827, 'learning_rate': 3.5e-06, 'epoch': 1.56}
{'loss': 3.8978, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.78}
{'loss': 3.9182, 'learning_rate': 4.5e-06, 'epoch': 2.0}
{'eval_loss': 3.696110725402832, 'eval_runtime': 28.0335, 'eval_samples_per_second': 35.672, 'eval_steps_per_second': 8.918, 'epoch': 2.0}
{'loss': 3.8124, 'learning_rate': 5e-06, 'epoch': 2.22}
{'loss': 3.7692, 'learning_rate': 5.500000000000001e-06, 'epoch': 2.44}
{'loss': 3.7412, 'learning_rate': 6e-06, 'epoch': 2.67}
{'loss': 3.7226, 'learning_rate': 6.5000000000000004e-06, 'epoch': 2.89}
{'eval_loss': 3.604734420776367, 'eval_runtime': 28.0268, 'eval_samples_per_second': 35.68, 'eval_steps_per_second': 8.92, 'epoch': 3.0}
{'loss': 3.7207, 'learning_rate': 7e-06, 'epoch': 3.11}
{'loss': 3.6332, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.33}
{'loss': 3.5987, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.56}
{'loss': 3.6078, 'learning_rate': 8.5e-06, 'epoch': 3.78}
{'loss': 3.6029, 'learning_rate': 9e-06, 'epoch': 4.0}
{'eval_loss': 3.572812557220459, 'eval_runtime': 28.0123, 'eval_samples_per_second': 35.699, 'eval_steps_per_second': 8.925, 'epoch': 4.0}
{'loss': 3.516, 'learning_rate': 9.5e-06, 'epoch': 4.22}
{'loss': 3.4912, 'learning_rate': 1e-05, 'epoch': 4.44}
{'loss': 3.5549, 'learning_rate': 9.600000000000001e-06, 'epoch': 4.67}
{'loss': 3.5168, 'learning_rate': 9.200000000000002e-06, 'epoch': 4.89}
{'eval_loss': 3.5551645755767822, 'eval_runtime': 28.0876, 'eval_samples_per_second': 35.603, 'eval_steps_per_second': 8.901, 'epoch': 5.0}
{'loss': 3.4507, 'learning_rate': 8.8e-06, 'epoch': 5.11}
{'loss': 3.4466, 'learning_rate': 8.400000000000001e-06, 'epoch': 5.33}
{'loss': 3.4241, 'learning_rate': 8.000000000000001e-06, 'epoch': 5.56}
{'loss': 3.4212, 'learning_rate': 7.600000000000001e-06, 'epoch': 5.78}
{'loss': 3.3909, 'learning_rate': 7.2000000000000005e-06, 'epoch': 6.0}
{'eval_loss': 3.552138566970825, 'eval_runtime': 28.0196, 'eval_samples_per_second': 35.689, 'eval_steps_per_second': 8.922, 'epoch': 6.0}
{'loss': 3.3625, 'learning_rate': 6.800000000000001e-06, 'epoch': 6.22}
{'loss': 3.3348, 'learning_rate': 6.4000000000000006e-06, 'epoch': 6.44}
{'loss': 3.3422, 'learning_rate': 6e-06, 'epoch': 6.67}
{'loss': 3.3678, 'learning_rate': 5.600000000000001e-06, 'epoch': 6.89}
{'eval_loss': 3.551982879638672, 'eval_runtime': 28.039, 'eval_samples_per_second': 35.665, 'eval_steps_per_second': 8.916, 'epoch': 7.0}
{'loss': 3.3024, 'learning_rate': 5.2e-06, 'epoch': 7.11}
{'loss': 3.2852, 'learning_rate': 4.800000000000001e-06, 'epoch': 7.33}
{'loss': 3.3164, 'learning_rate': 4.4e-06, 'epoch': 7.56}
{'loss': 3.2993, 'learning_rate': 4.000000000000001e-06, 'epoch': 7.78}
{'loss': 3.2826, 'learning_rate': 3.6000000000000003e-06, 'epoch': 8.0}
{'eval_loss': 3.550360679626465, 'eval_runtime': 27.9934, 'eval_samples_per_second': 35.723, 'eval_steps_per_second': 8.931, 'epoch': 8.0}
{'loss': 3.2297, 'learning_rate': 3.2000000000000003e-06, 'epoch': 8.22}
{'loss': 3.2717, 'learning_rate': 2.8000000000000003e-06, 'epoch': 8.44}
{'loss': 3.2944, 'learning_rate': 2.4000000000000003e-06, 'epoch': 8.67}
{'loss': 3.2367, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.89}
{'eval_loss': 3.5563418865203857, 'eval_runtime': 28.0673, 'eval_samples_per_second': 35.629, 'eval_steps_per_second': 8.907, 'epoch': 9.0}
{'loss': 3.2116, 'learning_rate': 1.6000000000000001e-06, 'epoch': 9.11}
{'loss': 3.2194, 'learning_rate': 1.2000000000000002e-06, 'epoch': 9.33}
{'loss': 3.2414, 'learning_rate': 8.000000000000001e-07, 'epoch': 9.56}
{'loss': 3.2331, 'learning_rate': 4.0000000000000003e-07, 'epoch': 9.78}
{'loss': 3.2117, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 3.559028148651123, 'eval_runtime': 28.0997, 'eval_samples_per_second': 35.588, 'eval_steps_per_second': 8.897, 'epoch': 10.0}
{'train_runtime': 7602.5858, 'train_samples_per_second': 11.835, 'train_steps_per_second': 2.96, 'train_loss': 3.597107796223958, 'epoch': 10.0}
  0%|          | 0/22500 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|██████████| 22500/22500 [2:06:35<00:00,  2.96it/s]