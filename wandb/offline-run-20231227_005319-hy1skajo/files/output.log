{'loss': 6.5668, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.22}
{'loss': 5.4625, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.44}
{'loss': 4.7253, 'learning_rate': 1.5e-06, 'epoch': 0.67}
{'loss': 4.4136, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.89}
{'eval_loss': 4.030318260192871, 'eval_runtime': 27.5162, 'eval_samples_per_second': 36.342, 'eval_steps_per_second': 9.086, 'epoch': 1.0}
{'loss': 4.2297, 'learning_rate': 2.5e-06, 'epoch': 1.11}
{'loss': 4.0656, 'learning_rate': 3e-06, 'epoch': 1.33}
{'loss': 3.9991, 'learning_rate': 3.5e-06, 'epoch': 1.56}
{'loss': 3.9701, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.78}
{'loss': 3.9276, 'learning_rate': 4.5e-06, 'epoch': 2.0}
{'eval_loss': 3.7856645584106445, 'eval_runtime': 27.5027, 'eval_samples_per_second': 36.36, 'eval_steps_per_second': 9.09, 'epoch': 2.0}
{'loss': 3.8239, 'learning_rate': 5e-06, 'epoch': 2.22}
{'loss': 3.8289, 'learning_rate': 5.500000000000001e-06, 'epoch': 2.44}
{'loss': 3.7754, 'learning_rate': 6e-06, 'epoch': 2.67}
{'loss': 3.7297, 'learning_rate': 6.5000000000000004e-06, 'epoch': 2.89}
{'eval_loss': 3.6860761642456055, 'eval_runtime': 27.4457, 'eval_samples_per_second': 36.436, 'eval_steps_per_second': 9.109, 'epoch': 3.0}
{'loss': 3.6803, 'learning_rate': 7e-06, 'epoch': 3.11}
{'loss': 3.6368, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.33}
{'loss': 3.6051, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.56}
{'loss': 3.6357, 'learning_rate': 8.5e-06, 'epoch': 3.78}
{'loss': 3.6356, 'learning_rate': 9e-06, 'epoch': 4.0}
{'eval_loss': 3.6433184146881104, 'eval_runtime': 27.4798, 'eval_samples_per_second': 36.39, 'eval_steps_per_second': 9.098, 'epoch': 4.0}
{'loss': 3.5644, 'learning_rate': 9.5e-06, 'epoch': 4.22}
{'loss': 3.5007, 'learning_rate': 1e-05, 'epoch': 4.44}
{'loss': 3.509, 'learning_rate': 9.600000000000001e-06, 'epoch': 4.67}
{'loss': 3.4939, 'learning_rate': 9.200000000000002e-06, 'epoch': 4.89}
{'eval_loss': 3.619203805923462, 'eval_runtime': 27.498, 'eval_samples_per_second': 36.366, 'eval_steps_per_second': 9.092, 'epoch': 5.0}
{'loss': 3.4934, 'learning_rate': 8.8e-06, 'epoch': 5.11}
{'loss': 3.3673, 'learning_rate': 8.400000000000001e-06, 'epoch': 5.33}
{'loss': 3.4494, 'learning_rate': 8.000000000000001e-06, 'epoch': 5.56}
{'loss': 3.3887, 'learning_rate': 7.600000000000001e-06, 'epoch': 5.78}
{'loss': 3.4121, 'learning_rate': 7.2000000000000005e-06, 'epoch': 6.0}
{'eval_loss': 3.6187617778778076, 'eval_runtime': 27.5619, 'eval_samples_per_second': 36.282, 'eval_steps_per_second': 9.07, 'epoch': 6.0}
{'loss': 3.312, 'learning_rate': 6.800000000000001e-06, 'epoch': 6.22}
{'loss': 3.3423, 'learning_rate': 6.4000000000000006e-06, 'epoch': 6.44}
{'loss': 3.3416, 'learning_rate': 6e-06, 'epoch': 6.67}
{'loss': 3.359, 'learning_rate': 5.600000000000001e-06, 'epoch': 6.89}
{'eval_loss': 3.6174216270446777, 'eval_runtime': 27.6523, 'eval_samples_per_second': 36.163, 'eval_steps_per_second': 9.041, 'epoch': 7.0}
{'loss': 3.2879, 'learning_rate': 5.2e-06, 'epoch': 7.11}
{'loss': 3.2489, 'learning_rate': 4.800000000000001e-06, 'epoch': 7.33}
{'loss': 3.2796, 'learning_rate': 4.4e-06, 'epoch': 7.56}
{'loss': 3.28, 'learning_rate': 4.000000000000001e-06, 'epoch': 7.78}
{'loss': 3.2994, 'learning_rate': 3.6000000000000003e-06, 'epoch': 8.0}
{'eval_loss': 3.6197867393493652, 'eval_runtime': 27.5915, 'eval_samples_per_second': 36.243, 'eval_steps_per_second': 9.061, 'epoch': 8.0}
{'loss': 3.2545, 'learning_rate': 3.2000000000000003e-06, 'epoch': 8.22}
{'loss': 3.2276, 'learning_rate': 2.8000000000000003e-06, 'epoch': 8.44}
{'loss': 3.2369, 'learning_rate': 2.4000000000000003e-06, 'epoch': 8.67}
{'loss': 3.2332, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.89}
{'eval_loss': 3.627225160598755, 'eval_runtime': 27.5928, 'eval_samples_per_second': 36.241, 'eval_steps_per_second': 9.06, 'epoch': 9.0}
{'loss': 3.2476, 'learning_rate': 1.6000000000000001e-06, 'epoch': 9.11}
{'loss': 3.2103, 'learning_rate': 1.2000000000000002e-06, 'epoch': 9.33}
{'loss': 3.2116, 'learning_rate': 8.000000000000001e-07, 'epoch': 9.56}
{'loss': 3.1907, 'learning_rate': 4.0000000000000003e-07, 'epoch': 9.78}
{'loss': 3.2267, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 3.6309731006622314, 'eval_runtime': 27.5753, 'eval_samples_per_second': 36.264, 'eval_steps_per_second': 9.066, 'epoch': 10.0}
{'train_runtime': 7529.1596, 'train_samples_per_second': 11.951, 'train_steps_per_second': 2.988, 'train_loss': 3.659562033420139, 'epoch': 10.0}
  0%|          | 0/22500 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|██████████| 22500/22500 [2:05:23<00:00,  2.99it/s]