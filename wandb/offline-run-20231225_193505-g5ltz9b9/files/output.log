{'loss': 4.9693, 'learning_rate': 1.25e-06, 'epoch': 0.22}
{'loss': 4.4575, 'learning_rate': 2.5e-06, 'epoch': 0.44}
{'loss': 4.1904, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.67}
{'loss': 4.0811, 'learning_rate': 5e-06, 'epoch': 0.89}
{'eval_loss': 3.770397424697876, 'eval_runtime': 28.5939, 'eval_samples_per_second': 34.972, 'eval_steps_per_second': 8.743, 'epoch': 1.0}
{'loss': 3.9358, 'learning_rate': 4.8780487804878055e-06, 'epoch': 1.11}
{'loss': 3.8587, 'learning_rate': 4.75609756097561e-06, 'epoch': 1.33}
{'loss': 3.822, 'learning_rate': 4.634146341463416e-06, 'epoch': 1.56}
{'loss': 3.7764, 'learning_rate': 4.51219512195122e-06, 'epoch': 1.78}
{'loss': 3.8197, 'learning_rate': 4.390243902439025e-06, 'epoch': 2.0}
{'eval_loss': 3.6332955360412598, 'eval_runtime': 28.3793, 'eval_samples_per_second': 35.237, 'eval_steps_per_second': 8.809, 'epoch': 2.0}
{'loss': 3.717, 'learning_rate': 4.268292682926829e-06, 'epoch': 2.22}
{'loss': 3.6869, 'learning_rate': 4.146341463414634e-06, 'epoch': 2.44}
{'loss': 3.6767, 'learning_rate': 4.024390243902439e-06, 'epoch': 2.67}
{'loss': 3.6679, 'learning_rate': 3.902439024390244e-06, 'epoch': 2.89}
{'eval_loss': 3.591644048690796, 'eval_runtime': 28.4631, 'eval_samples_per_second': 35.133, 'eval_steps_per_second': 8.783, 'epoch': 3.0}
{'loss': 3.6825, 'learning_rate': 3.780487804878049e-06, 'epoch': 3.11}
{'loss': 3.6084, 'learning_rate': 3.6585365853658537e-06, 'epoch': 3.33}
{'loss': 3.5795, 'learning_rate': 3.5365853658536588e-06, 'epoch': 3.56}
{'loss': 3.5946, 'learning_rate': 3.414634146341464e-06, 'epoch': 3.78}
{'loss': 3.5945, 'learning_rate': 3.292682926829269e-06, 'epoch': 4.0}
{'eval_loss': 3.5787079334259033, 'eval_runtime': 30.4525, 'eval_samples_per_second': 32.838, 'eval_steps_per_second': 8.21, 'epoch': 4.0}
{'loss': 3.5474, 'learning_rate': 3.1707317073170736e-06, 'epoch': 4.22}
{'loss': 3.5223, 'learning_rate': 3.0487804878048782e-06, 'epoch': 4.44}
{'loss': 3.5866, 'learning_rate': 2.926829268292683e-06, 'epoch': 4.67}
{'loss': 3.5514, 'learning_rate': 2.8048780487804884e-06, 'epoch': 4.89}
{'eval_loss': 3.573218584060669, 'eval_runtime': 32.0888, 'eval_samples_per_second': 31.163, 'eval_steps_per_second': 7.791, 'epoch': 5.0}
{'loss': 3.5161, 'learning_rate': 2.682926829268293e-06, 'epoch': 5.11}
{'loss': 3.5373, 'learning_rate': 2.5609756097560977e-06, 'epoch': 5.33}
{'loss': 3.5133, 'learning_rate': 2.4390243902439027e-06, 'epoch': 5.56}
{'loss': 3.5087, 'learning_rate': 2.317073170731708e-06, 'epoch': 5.78}
{'loss': 3.4794, 'learning_rate': 2.1951219512195125e-06, 'epoch': 6.0}
{'eval_loss': 3.5737149715423584, 'eval_runtime': 34.1551, 'eval_samples_per_second': 29.278, 'eval_steps_per_second': 7.32, 'epoch': 6.0}
{'loss': 3.5003, 'learning_rate': 2.073170731707317e-06, 'epoch': 6.22}
{'loss': 3.4727, 'learning_rate': 1.951219512195122e-06, 'epoch': 6.44}
{'loss': 3.4754, 'learning_rate': 1.8292682926829268e-06, 'epoch': 6.67}
{'loss': 3.502, 'learning_rate': 1.707317073170732e-06, 'epoch': 6.89}
{'eval_loss': 3.572342872619629, 'eval_runtime': 32.3276, 'eval_samples_per_second': 30.933, 'eval_steps_per_second': 7.733, 'epoch': 7.0}
{'loss': 3.4538, 'learning_rate': 1.5853658536585368e-06, 'epoch': 7.11}
{'loss': 3.4542, 'learning_rate': 1.4634146341463414e-06, 'epoch': 7.33}
{'loss': 3.4846, 'learning_rate': 1.3414634146341465e-06, 'epoch': 7.56}
{'loss': 3.4669, 'learning_rate': 1.2195121951219514e-06, 'epoch': 7.78}
{'loss': 3.4477, 'learning_rate': 1.0975609756097562e-06, 'epoch': 8.0}
{'eval_loss': 3.570671796798706, 'eval_runtime': 28.6596, 'eval_samples_per_second': 34.892, 'eval_steps_per_second': 8.723, 'epoch': 8.0}
{'loss': 3.4254, 'learning_rate': 9.75609756097561e-07, 'epoch': 8.22}
{'loss': 3.4675, 'learning_rate': 8.53658536585366e-07, 'epoch': 8.44}
{'loss': 3.4881, 'learning_rate': 7.317073170731707e-07, 'epoch': 8.67}
{'loss': 3.4275, 'learning_rate': 6.097560975609757e-07, 'epoch': 8.89}
{'eval_loss': 3.5715603828430176, 'eval_runtime': 28.3065, 'eval_samples_per_second': 35.328, 'eval_steps_per_second': 8.832, 'epoch': 9.0}
{'loss': 3.4117, 'learning_rate': 4.878048780487805e-07, 'epoch': 9.11}
{'loss': 3.4322, 'learning_rate': 3.6585365853658536e-07, 'epoch': 9.33}
{'loss': 3.4513, 'learning_rate': 2.439024390243903e-07, 'epoch': 9.56}
{'loss': 3.4426, 'learning_rate': 1.2195121951219514e-07, 'epoch': 9.78}
{'loss': 3.4196, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 3.571683168411255, 'eval_runtime': 28.1164, 'eval_samples_per_second': 35.566, 'eval_steps_per_second': 8.892, 'epoch': 10.0}
{'train_runtime': 7860.9846, 'train_samples_per_second': 11.446, 'train_steps_per_second': 2.862, 'train_loss': 3.6378854817708333, 'epoch': 10.0}
  0%|          | 0/22500 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|██████████| 22500/22500 [2:10:51<00:00,  2.87it/s]