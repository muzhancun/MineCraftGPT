{'loss': 5.0652, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.22}
{'loss': 4.6485, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.44}
{'loss': 4.411, 'learning_rate': 1.5e-06, 'epoch': 0.67}
{'loss': 4.2552, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.89}
{'eval_loss': 3.961923837661743, 'eval_runtime': 27.8711, 'eval_samples_per_second': 35.88, 'eval_steps_per_second': 8.97, 'epoch': 1.0}
{'loss': 4.167, 'learning_rate': 2.5e-06, 'epoch': 1.11}
{'loss': 4.0576, 'learning_rate': 3e-06, 'epoch': 1.33}
{'loss': 3.952, 'learning_rate': 3.5e-06, 'epoch': 1.56}
{'loss': 3.8929, 'learning_rate': 4.000000000000001e-06, 'epoch': 1.78}
{'loss': 3.903, 'learning_rate': 4.5e-06, 'epoch': 2.0}
{'eval_loss': 3.7260241508483887, 'eval_runtime': 27.6748, 'eval_samples_per_second': 36.134, 'eval_steps_per_second': 9.033, 'epoch': 2.0}
{'loss': 3.7721, 'learning_rate': 5e-06, 'epoch': 2.22}
{'loss': 3.7967, 'learning_rate': 5.500000000000001e-06, 'epoch': 2.44}
{'loss': 3.7717, 'learning_rate': 6e-06, 'epoch': 2.67}
{'loss': 3.7386, 'learning_rate': 6.5000000000000004e-06, 'epoch': 2.89}
{'eval_loss': 3.6466596126556396, 'eval_runtime': 30.2887, 'eval_samples_per_second': 33.016, 'eval_steps_per_second': 8.254, 'epoch': 3.0}
{'loss': 3.6799, 'learning_rate': 7e-06, 'epoch': 3.11}
{'loss': 3.6076, 'learning_rate': 7.500000000000001e-06, 'epoch': 3.33}
{'loss': 3.6057, 'learning_rate': 8.000000000000001e-06, 'epoch': 3.56}
{'loss': 3.6296, 'learning_rate': 8.5e-06, 'epoch': 3.78}
{'loss': 3.6018, 'learning_rate': 9e-06, 'epoch': 4.0}
{'eval_loss': 3.6078765392303467, 'eval_runtime': 27.8622, 'eval_samples_per_second': 35.891, 'eval_steps_per_second': 8.973, 'epoch': 4.0}
{'loss': 3.5189, 'learning_rate': 9.5e-06, 'epoch': 4.22}
{'loss': 3.5626, 'learning_rate': 1e-05, 'epoch': 4.44}
{'loss': 3.5235, 'learning_rate': 9.600000000000001e-06, 'epoch': 4.67}
{'loss': 3.4685, 'learning_rate': 9.200000000000002e-06, 'epoch': 4.89}
{'eval_loss': 3.595771551132202, 'eval_runtime': 27.719, 'eval_samples_per_second': 36.076, 'eval_steps_per_second': 9.019, 'epoch': 5.0}
{'loss': 3.4258, 'learning_rate': 8.8e-06, 'epoch': 5.11}
{'loss': 3.4405, 'learning_rate': 8.400000000000001e-06, 'epoch': 5.33}
{'loss': 3.3967, 'learning_rate': 8.000000000000001e-06, 'epoch': 5.56}
{'loss': 3.4662, 'learning_rate': 7.600000000000001e-06, 'epoch': 5.78}
{'loss': 3.3881, 'learning_rate': 7.2000000000000005e-06, 'epoch': 6.0}
{'eval_loss': 3.584397315979004, 'eval_runtime': 29.8239, 'eval_samples_per_second': 33.53, 'eval_steps_per_second': 8.383, 'epoch': 6.0}
{'loss': 3.3243, 'learning_rate': 6.800000000000001e-06, 'epoch': 6.22}
{'loss': 3.3405, 'learning_rate': 6.4000000000000006e-06, 'epoch': 6.44}
{'loss': 3.3556, 'learning_rate': 6e-06, 'epoch': 6.67}
{'loss': 3.3499, 'learning_rate': 5.600000000000001e-06, 'epoch': 6.89}
{'eval_loss': 3.5872368812561035, 'eval_runtime': 27.6402, 'eval_samples_per_second': 36.179, 'eval_steps_per_second': 9.045, 'epoch': 7.0}
{'loss': 3.3106, 'learning_rate': 5.2e-06, 'epoch': 7.11}
{'loss': 3.3042, 'learning_rate': 4.800000000000001e-06, 'epoch': 7.33}
{'loss': 3.2758, 'learning_rate': 4.4e-06, 'epoch': 7.56}
{'loss': 3.2988, 'learning_rate': 4.000000000000001e-06, 'epoch': 7.78}
{'loss': 3.2932, 'learning_rate': 3.6000000000000003e-06, 'epoch': 8.0}
{'eval_loss': 3.5888783931732178, 'eval_runtime': 28.0259, 'eval_samples_per_second': 35.681, 'eval_steps_per_second': 8.92, 'epoch': 8.0}
{'loss': 3.2717, 'learning_rate': 3.2000000000000003e-06, 'epoch': 8.22}
{'loss': 3.2655, 'learning_rate': 2.8000000000000003e-06, 'epoch': 8.44}
{'loss': 3.208, 'learning_rate': 2.4000000000000003e-06, 'epoch': 8.67}
{'loss': 3.2701, 'learning_rate': 2.0000000000000003e-06, 'epoch': 8.89}
{'eval_loss': 3.591914415359497, 'eval_runtime': 27.8459, 'eval_samples_per_second': 35.912, 'eval_steps_per_second': 8.978, 'epoch': 9.0}
{'loss': 3.2023, 'learning_rate': 1.6000000000000001e-06, 'epoch': 9.11}
{'loss': 3.196, 'learning_rate': 1.2000000000000002e-06, 'epoch': 9.33}
{'loss': 3.2471, 'learning_rate': 8.000000000000001e-07, 'epoch': 9.56}
{'loss': 3.2146, 'learning_rate': 4.0000000000000003e-07, 'epoch': 9.78}
{'loss': 3.2351, 'learning_rate': 0.0, 'epoch': 10.0}
{'eval_loss': 3.595580577850342, 'eval_runtime': 27.8319, 'eval_samples_per_second': 35.93, 'eval_steps_per_second': 8.982, 'epoch': 10.0}
{'train_runtime': 7628.5681, 'train_samples_per_second': 11.795, 'train_steps_per_second': 2.949, 'train_loss': 3.593556494140625, 'epoch': 10.0}
  0%|          | 0/22500 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|██████████| 22500/22500 [2:06:59<00:00,  2.95it/s]